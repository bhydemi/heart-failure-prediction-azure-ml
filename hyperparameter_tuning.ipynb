{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Failure Prediction using HyperDrive\n",
    "\n",
    "This notebook demonstrates hyperparameter tuning using Azure HyperDrive\n",
    "for a Logistic Regression model to predict heart failure mortality.\n",
    "\n",
    "## Overview\n",
    "1. Setup workspace and compute\n",
    "2. Load and register the dataset\n",
    "3. Configure HyperDrive with hyperparameter search space\n",
    "4. Run HyperDrive experiment\n",
    "5. Analyze results and retrieve best model\n",
    "6. Register and deploy the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from azureml.core import Workspace, Experiment, Dataset, Environment, ScriptRunConfig\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.hyperdrive import (\n",
    "    HyperDriveConfig,\n",
    "    RandomParameterSampling,\n",
    "    BanditPolicy,\n",
    "    choice,\n",
    "    uniform,\n",
    "    loguniform,\n",
    "    PrimaryMetricGoal\n",
    ")\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.model import Model, InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the workspace\n",
    "ws = Workspace.from_config()\n",
    "print(f\"Workspace name: {ws.name}\")\n",
    "print(f\"Subscription ID: {ws.subscription_id}\")\n",
    "print(f\"Resource group: {ws.resource_group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compute cluster name\n",
    "compute_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    # Check if the compute target already exists\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "    print(f\"Found existing compute target: {compute_name}\")\n",
    "except ComputeTargetException:\n",
    "    # Create a new compute cluster\n",
    "    print(f\"Creating new compute cluster: {compute_name}\")\n",
    "    \n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_D2_V2\",\n",
    "        max_nodes=4,\n",
    "        min_nodes=0\n",
    "    )\n",
    "    \n",
    "    compute_target = ComputeTarget.create(ws, compute_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "print(f\"Compute target status: {compute_target.get_status().serialize()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Register Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the local dataset\n",
    "df = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "print(f\"\\nTarget variable distribution:\")\n",
    "print(df['DEATH_EVENT'].value_counts())\n",
    "print(f\"\\nDataset statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset is already registered, if not register it\n",
    "try:\n",
    "    dataset = Dataset.get_by_name(ws, name='heart-failure-dataset')\n",
    "    print(\"Dataset already registered.\")\n",
    "except Exception:\n",
    "    # Get the default datastore\n",
    "    datastore = ws.get_default_datastore()\n",
    "    \n",
    "    # Upload the dataset to the datastore\n",
    "    datastore.upload_files(\n",
    "        files=['heart_failure_clinical_records_dataset.csv'],\n",
    "        target_path='heart-failure-data/',\n",
    "        overwrite=True,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    # Create and register a TabularDataset\n",
    "    dataset = Dataset.Tabular.from_delimited_files(\n",
    "        path=(datastore, 'heart-failure-data/heart_failure_clinical_records_dataset.csv')\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.register(\n",
    "        workspace=ws,\n",
    "        name='heart-failure-dataset',\n",
    "        description='Heart Failure Clinical Records Dataset from Kaggle',\n",
    "        create_new_version=True\n",
    "    )\n",
    "    print(f\"Dataset registered: {dataset.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python environment for the training script\n",
    "env = Environment.from_conda_specification(\n",
    "    name='heart-failure-env',\n",
    "    file_path='conda_env.yml'\n",
    ")\n",
    "\n",
    "# Register the environment\n",
    "env.register(workspace=ws)\n",
    "print(f\"Environment registered: {env.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure HyperDrive\n",
    "\n",
    "### Hyperparameter Search Space\n",
    "\n",
    "We are tuning the following hyperparameters for Logistic Regression:\n",
    "\n",
    "1. **C (Regularization Strength)**: Inverse of regularization strength. Smaller values = stronger regularization.\n",
    "   - Range: 0.001 to 100 (log-uniform distribution)\n",
    "   - Rationale: Using log-uniform sampling because regularization strength often works better on a logarithmic scale\n",
    "\n",
    "2. **max_iter (Maximum Iterations)**: Maximum number of iterations for the solver to converge.\n",
    "   - Values: 50, 100, 150, 200, 300\n",
    "   - Rationale: Discrete choices to ensure convergence while not over-iterating\n",
    "\n",
    "3. **solver (Optimization Algorithm)**: Algorithm to use for optimization.\n",
    "   - Values: 'lbfgs', 'liblinear', 'saga'\n",
    "   - Rationale: Different solvers work better for different data characteristics\n",
    "\n",
    "### Sampling Method: Random Sampling\n",
    "- Random sampling is chosen because it's more efficient than grid search for exploring the hyperparameter space\n",
    "- It can find good configurations faster, especially with continuous parameters\n",
    "\n",
    "### Early Termination Policy: Bandit Policy\n",
    "- Terminates runs that are not performing well compared to the best run\n",
    "- slack_factor=0.1 means runs with accuracy < 90% of the best run will be terminated\n",
    "- evaluation_interval=2 means the policy is applied every 2 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space\n",
    "param_sampling = RandomParameterSampling({\n",
    "    '--C': loguniform(-3, 2),  # Regularization: 0.001 to 100\n",
    "    '--max_iter': choice(50, 100, 150, 200, 300),\n",
    "    '--solver': choice('lbfgs', 'liblinear', 'saga')\n",
    "})\n",
    "\n",
    "print(\"Parameter sampling configured:\")\n",
    "print(\"  - C: loguniform(-3, 2) -> [0.001, 100]\")\n",
    "print(\"  - max_iter: choice(50, 100, 150, 200, 300)\")\n",
    "print(\"  - solver: choice('lbfgs', 'liblinear', 'saga')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early termination policy\n",
    "early_termination_policy = BanditPolicy(\n",
    "    slack_factor=0.1,\n",
    "    evaluation_interval=2,\n",
    "    delay_evaluation=5\n",
    ")\n",
    "\n",
    "print(\"Early termination policy configured:\")\n",
    "print(\"  - Type: Bandit Policy\")\n",
    "print(\"  - Slack factor: 0.1 (terminate if accuracy < 90% of best)\")\n",
    "print(\"  - Evaluation interval: 2\")\n",
    "print(\"  - Delay evaluation: 5 (start evaluating after 5 iterations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ScriptRunConfig\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory='.',\n",
    "    script='train.py',\n",
    "    compute_target=compute_target,\n",
    "    environment=env\n",
    ")\n",
    "\n",
    "print(\"Script run configuration created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HyperDrive configuration\n",
    "hyperdrive_config = HyperDriveConfig(\n",
    "    run_config=script_config,\n",
    "    hyperparameter_sampling=param_sampling,\n",
    "    policy=early_termination_policy,\n",
    "    primary_metric_name='Accuracy',\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs=20,\n",
    "    max_concurrent_runs=4\n",
    ")\n",
    "\n",
    "print(\"HyperDrive configuration created:\")\n",
    "print(\"  - Primary metric: Accuracy (maximize)\")\n",
    "print(\"  - Max total runs: 20\")\n",
    "print(\"  - Max concurrent runs: 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run HyperDrive Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the experiment\n",
    "experiment = Experiment(ws, \"heart-failure-hyperdrive\")\n",
    "\n",
    "# Submit the HyperDrive run\n",
    "print(\"Submitting HyperDrive experiment...\")\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)\n",
    "print(f\"Run ID: {hyperdrive_run.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the RunDetails widget to monitor progress\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the run to complete\n",
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Retrieve and Analyze Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best run\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "\n",
    "# Display best run details\n",
    "print(f\"Best Run ID: {best_run.id}\")\n",
    "print(f\"\\nBest Run Metrics:\")\n",
    "\n",
    "# Get metrics\n",
    "metrics = best_run.get_metrics()\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"  {metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_run_params = best_run.get_details()['runDefinition']['arguments']\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for i in range(0, len(best_run_params), 2):\n",
    "    print(f\"  {best_run_params[i]}: {best_run_params[i+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all child runs with their metrics\n",
    "print(\"\\nAll HyperDrive Runs:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "child_runs = list(hyperdrive_run.get_children())\n",
    "for run in sorted(child_runs, key=lambda r: r.get_metrics().get('Accuracy', 0), reverse=True)[:10]:\n",
    "    metrics = run.get_metrics()\n",
    "    params = run.get_details()['runDefinition']['arguments']\n",
    "    print(f\"Run ID: {run.id}\")\n",
    "    print(f\"  Accuracy: {metrics.get('Accuracy', 'N/A')}\")\n",
    "    print(f\"  Parameters: {params}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Register the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the best model\n",
    "model_path = best_run.download_file('outputs/model.joblib', output_file_path='outputs/model.joblib')\n",
    "print(f\"Model downloaded to: outputs/model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the best model\n",
    "model_name = 'heart-failure-hyperdrive-model'\n",
    "\n",
    "# Get the best hyperparameters for tags\n",
    "best_params = {}\n",
    "args = best_run.get_details()['runDefinition']['arguments']\n",
    "for i in range(0, len(args), 2):\n",
    "    best_params[args[i].replace('--', '')] = args[i+1]\n",
    "\n",
    "registered_model = best_run.register_model(\n",
    "    model_name=model_name,\n",
    "    model_path='outputs/model.joblib',\n",
    "    description='Heart Failure Prediction Model trained with HyperDrive',\n",
    "    tags={\n",
    "        'algorithm': 'LogisticRegression',\n",
    "        'accuracy': str(metrics.get('Accuracy', 'N/A')),\n",
    "        'C': str(best_params.get('C', 'N/A')),\n",
    "        'max_iter': str(best_params.get('max_iter', 'N/A')),\n",
    "        'solver': str(best_params.get('solver', 'N/A'))\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model registered: {registered_model.name}\")\n",
    "print(f\"Model version: {registered_model.version}\")\n",
    "print(f\"Model ID: {registered_model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure inference\n",
    "inference_config = InferenceConfig(\n",
    "    entry_script='score.py',\n",
    "    environment=env\n",
    ")\n",
    "\n",
    "# Configure the ACI deployment\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=1,\n",
    "    memory_gb=1,\n",
    "    auth_enabled=True,\n",
    "    enable_app_insights=True,\n",
    "    description='Heart Failure Prediction Service (HyperDrive)'\n",
    ")\n",
    "\n",
    "print(\"Deployment configuration created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "service_name = 'heart-failure-hd-service'\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[registered_model],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=aci_config,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n",
    "print(f\"\\nService state: {service.state}\")\n",
    "print(f\"Scoring URI: {service.scoring_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test the Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get the scoring URI and keys\n",
    "scoring_uri = service.scoring_uri\n",
    "primary_key, secondary_key = service.get_keys()\n",
    "\n",
    "# Prepare sample data for testing\n",
    "# Feature order: age, anaemia, creatinine_phosphokinase, diabetes, ejection_fraction,\n",
    "#                high_blood_pressure, platelets, serum_creatinine, serum_sodium, sex, smoking, time\n",
    "sample_data = {\n",
    "    \"data\": [\n",
    "        [75, 0, 582, 0, 20, 1, 265000, 1.9, 130, 1, 0, 4],   # Expected: DEATH_EVENT=1\n",
    "        [55, 0, 7861, 0, 38, 0, 263358.03, 1.1, 136, 1, 0, 6],  # Expected: DEATH_EVENT=1\n",
    "        [45, 0, 2060, 1, 60, 0, 742000, 0.8, 138, 0, 0, 278]  # Expected: DEATH_EVENT=0\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Set the headers\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {primary_key}'\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.post(scoring_uri, json=sample_data, headers=headers)\n",
    "\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(f\"Response: {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare with AutoML Model\n",
    "\n",
    "After running both notebooks, compare the results:\n",
    "\n",
    "| Metric | AutoML Model | HyperDrive Model |\n",
    "|--------|--------------|------------------|\n",
    "| Accuracy | [Fill after running] | [Fill after running] |\n",
    "| Algorithm | [Auto-selected] | Logistic Regression |\n",
    "| Training Time | [Fill after running] | [Fill after running] |\n",
    "\n",
    "The best model should be deployed based on the comparison results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print service logs for debugging\n",
    "print(\"Service Logs:\")\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the web service (uncomment to run)\n",
    "# service.delete()\n",
    "# print(\"Service deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the compute cluster (uncomment to run)\n",
    "# compute_target.delete()\n",
    "# print(\"Compute cluster deleted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
